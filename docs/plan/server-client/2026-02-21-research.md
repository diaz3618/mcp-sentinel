# MCP Sentinel Server/Client Separation Research

Date: 2026-02-21  
Repo: `/home/diaz/git/MCP-Gateway`  
Scope: Evaluate splitting MCP Sentinel into a standalone server + separate Textual client, with secure management access and multi-server client control.

## 1) Research Method

### Local code inspection (no assumptions)

Inspected these runtime-critical modules directly:

- `mcp_sentinel/cli.py`
- `mcp_sentinel/server/app.py`
- `mcp_sentinel/server/lifespan.py`
- `mcp_sentinel/server/transport.py`
- `mcp_sentinel/server/handlers.py`
- `mcp_sentinel/bridge/client_manager.py`
- `mcp_sentinel/bridge/capability_registry.py`
- `mcp_sentinel/bridge/forwarder.py`
- `mcp_sentinel/display/console.py`
- `mcp_sentinel/tui/app.py`
- `mcp_sentinel/config/loader.py`

### External research approach

Used primary sources only (official protocol/docs/repos) and refreshed to latest available pages on 2026-02-21.

## 2) Verified Current Architecture

### Process model

- `--no-tui` starts Uvicorn directly on the main thread (`mcp_sentinel/cli.py`).
- Default mode starts Textual (`SentinelApp`) and launches Uvicorn in a daemon thread (`mcp_sentinel/tui/app.py`).

This means in default mode, server lifecycle is coupled to the TUI process.

### Exposed network API today

- Starlette routes expose MCP transport endpoints only:
  - `GET /sse`
  - `POST /messages/`
- Implemented via `mcp_sentinel/server/app.py` and `mcp_sentinel/server/transport.py`.

There is no management API endpoint for health, backend inventory, capability metadata, or lifecycle operations.

### TUI coupling pattern

- TUI receives server lifecycle updates through in-process callback plumbing:
  - `set_status_callback(...)` / `disp_console_status(...)`.
- Status payloads include Python objects (tool/resource/prompt instances), not a wire contract.

This confirms the current TUI is not a remote client.

### Backend/control logic placement

Core backend logic is already server-side:

- Backend session/process orchestration: `bridge/client_manager.py`
- Capability aggregation/routing: `bridge/capability_registry.py`
- MCP request forwarding: `bridge/forwarder.py`

This is a strong base for introducing a separate control plane API without rewriting core behavior.

### Current constraints discovered

- No automated test suite for core package was found in this repo.
- `README.md` structure description is stale vs current `mcp_sentinel/*` layout.
- Lifespan currently does startup/discovery/attach in one path (`server/lifespan.py`), which complicates adding runtime control operations (reload/reconnect) unless refactored.

## 3) Does Server/Client Separation Make Sense Here?

Yes.

Based on verified code structure, separation is a good fit because:

- Server behavior already lives in non-TUI modules.
- Current coupling pain is process ownership and UI callback transport, not protocol logic.
- A control plane API can be added with limited disruption to existing MCP data plane.

This directly addresses your goals:

- server runs independently of TUI,
- TUI can manage one or many servers,
- lifecycle operations move to server-side management API.

## 4) Protocol and Ecosystem Findings (Up-to-date)

### MCP protocol (latest revision)

- Latest published MCP spec revision includes `2025-11-25` (newer than `2025-06-18`).
- Transport guidance includes Streamable HTTP / HTTP+SSE expectations and security requirements such as origin validation, localhost binding when local, and authentication.

Implication: building a standalone network server and separate management channel is aligned with current MCP direction.

### MCP Python SDK status

- Official `python-sdk` release notes show `v1.23.1` supporting protocol revision `2025-11-05` and the `2025-11-25` spec release.

Implication: Python-first implementation is current and viable for this split.

### Textual suitability as remote client

- Official Textual docs recommend using workers for concurrent/background tasks.

Implication: a Textual client can safely handle non-blocking network calls (polling/SSE/WebSocket control stream) without tying server lifetime to UI thread ownership.

### Starlette support for lifecycle and composition

- Official Starlette docs support explicit lifespan handling and mounting sub-applications.

Implication: adding a management sub-app/router beside existing MCP transport routes is a natural fit for the current framework.

## 5) Security Findings for Management Plane

Your security concern is valid. A separate management port increases control-plane attack surface and should be explicitly hardened.

Minimum safe baseline for this project:

1. Bind management API to `127.0.0.1` by default.
2. Require bearer token auth for management endpoints (except optional unauthenticated local health if explicitly enabled).
3. Support TLS for remote management; support mTLS as an optional stricter mode.
4. Separate authorization scopes (`read` vs `admin`) for destructive actions.
5. Emit audit logs for management actions (actor, endpoint, target, result).
6. Keep data-plane MCP transport and control-plane management auth independent.

## 6) Rust and Go as Parallel Server Implementations (Not Replacement)

### Rust

Pros:

- Strong memory safety and ownership guarantees.
- Strong compile-time concurrency model (`Send`/`Sync`).
- High-performance async ecosystem (`tokio`, `hyper`) suitable for high concurrency and strict reliability.

Cons:

- Higher engineering complexity and onboarding cost.
- Slower delivery if team is primarily Python-oriented.
- Duplicated protocol/business logic if Python remains canonical implementation.

Best fit for this repo: parallel implementation after API contracts and operational requirements are stable.

### Go

Pros:

- Excellent standard HTTP server tooling (`net/http`).
- Simple deployment/operations model.
- Strong tooling for concurrency validation/profiling (`-race`, `pprof`).

Cons:

- Concurrency correctness relies more on discipline/tooling than Rust-level compile-time guarantees.
- GC/runtime behavior may need tuning for certain low-latency workloads.

Best fit for this repo: pragmatic parallel implementation when service velocity and operational simplicity are priority.

## 7) Research Conclusion

- Server/client split is technically justified for MCP Sentinel as implemented today.
- Recommended path: implement separation in Python first, define stable management API contracts, then evaluate Rust/Go parallel server implementations against measurable needs.
- The primary blocker is not capability routing logic; it is introducing an explicit runtime/control-plane abstraction and replacing in-process TUI callback coupling.

## Sources (Accessed 2026-02-21)

- MCP Spec (latest revision index / 2025-11-25): https://modelcontextprotocol.io/specification/2025-11-25/basic/transports
- MCP Spec (2025-06-18): https://modelcontextprotocol.io/specification/2025-06-18/basic/transports
- MCP Python SDK repo: https://github.com/modelcontextprotocol/python-sdk
- MCP Python SDK release `v1.23.1`: https://github.com/modelcontextprotocol/python-sdk/releases/tag/v1.23.1
- Textual docs (Workers Guide): https://textual.textualize.io/guide/workers/
- Starlette docs (Lifespan): https://www.starlette.io/lifespan/
- Starlette docs (Routing / Mount): https://www.starlette.io/routing/
- Rust Book (Ownership): https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html
- Rust Book (Concurrency): https://doc.rust-lang.org/book/ch16-00-concurrency.html
- Rust Book (Send/Sync): https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html
- Tokio runtime docs: https://docs.rs/tokio/latest/tokio/runtime/
- Hyper: https://hyper.rs/
- Go `net/http`: https://pkg.go.dev/net/http
- Go race detector: https://go.dev/doc/articles/race_detector.html
- Go memory model: https://go.dev/ref/mem
- Go `net/http/pprof`: https://pkg.go.dev/net/http/pprof
